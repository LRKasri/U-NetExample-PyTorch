{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd to the script folder\n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "os.chdir('drive/My Drive/U-NetExample-master')#Need to change the address to where your script is\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.name_list = os.listdir(img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.name_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, f'{idx+1}.tif')\n",
    "        img = Image.open(img_path).convert('RGB')  # Convert to grayscale\n",
    "        img = np.array(img).astype('float32')\n",
    "        img = np.array(img)\n",
    "        # Normalize image\n",
    "        img -= img.mean()\n",
    "        img /= img.std()\n",
    "        # img = img[np.newaxis, ...]  # Add channel dimension\n",
    "\n",
    "        # Load label\n",
    "        label_path = os.path.join(self.label_dir, f'{idx+1}.tif')\n",
    "        label = Image.open(label_path).convert('L')\n",
    "        label = np.array(label).astype('float32') / 255.0\n",
    "        label = np.where(label > 0.5, 0.0, 1.0)  # Inverting labels as in original code\n",
    "        # label = label.reshape(-1, 1)  # Reshape to (-1, 1)\n",
    "\n",
    "        # Convert to tensors\n",
    "        img_tensor = torch.from_numpy(img).float()\n",
    "        label_tensor = torch.from_numpy(label).long()\n",
    "\n",
    "        return img_tensor, label_tensor\n",
    "\n",
    "# Usage example:\n",
    "train_dataset = TrainDataset('data/train', 'data/label')\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kasrirb/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kasrirb/miniconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initiaization of the model, here we use ResNet18 as the encoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def up_conv(in_channels, out_channels):\n",
    "    return nn.ConvTranspose2d(\n",
    "        in_channels, out_channels, kernel_size=2, stride=2\n",
    "    )\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.encoder = models.resnet18(pretrained=True)\n",
    "        self.encoder_layers = list(self.encoder.children())\n",
    "\n",
    "        self.block1 = nn.Sequential(*self.encoder_layers[:3])\n",
    "        self.block2 = nn.Sequential(*self.encoder_layers[3:5])\n",
    "        self.block3 = self.encoder_layers[5]\n",
    "        self.block4 = self.encoder_layers[6]\n",
    "        self.block5 = self.encoder_layers[7]\n",
    "\n",
    "        self.up_conv6 = up_conv(512, 512)\n",
    "        self.conv6 = double_conv(512 + 256, 512)\n",
    "        self.up_conv7 = up_conv(512, 256)\n",
    "        self.conv7 = double_conv(256 + 128, 256)\n",
    "        self.up_conv8 = up_conv(256, 128)\n",
    "        self.conv8 = double_conv(128 + 64, 128)\n",
    "        self.up_conv9 = up_conv(128, 64)\n",
    "        self.conv9 = double_conv(64 + 64, 64)\n",
    "        self.up_conv10 = up_conv(64, 32)\n",
    "        self.conv10 = nn.Conv2d(32, n_classes, kernel_size=1)\n",
    "        # self.softmax = nn.Softmax(dim=1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        block1 = self.block1(x)\n",
    "        block2 = self.block2(block1)\n",
    "        block3 = self.block3(block2)\n",
    "        block4 = self.block4(block3)\n",
    "        block5 = self.block5(block4)\n",
    "\n",
    "        x = self.up_conv6(block5)\n",
    "        x = torch.cat([x, block4], dim=1)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = self.up_conv7(x)\n",
    "        x = torch.cat([x, block3], dim=1)\n",
    "        x = self.conv7(x)\n",
    "\n",
    "        x = self.up_conv8(x)\n",
    "        x = torch.cat([x, block2], dim=1)\n",
    "        x = self.conv8(x)\n",
    "\n",
    "        x = self.up_conv9(x)\n",
    "        x = torch.cat([x, block1], dim=1)\n",
    "        x = self.conv9(x)\n",
    "\n",
    "        x = self.up_conv10(x)\n",
    "        x = self.conv10(x)\n",
    "        # x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = UNet(n_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def flatten(tensor):\n",
    "    \"\"\"Flattens a given tensor such that the channel axis is first.\n",
    "    The shapes are transformed as follows:\n",
    "       (N, C, D, H, W) -> (C, N * D * H * W)\n",
    "    \"\"\"\n",
    "    # number of channels\n",
    "    C = tensor.size(1)\n",
    "    # new axis order\n",
    "    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n",
    "    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n",
    "    transposed = tensor.permute(axis_order)\n",
    "    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n",
    "    return transposed.contiguous().view(C, -1)\n",
    "\n",
    "class MyCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCriterion, self).__init__()\n",
    "        self.epsilon = 1e-6\n",
    "\n",
    "    def dice_loss(self, inputs, targets):\n",
    "        # Assuming inputs are raw logits and have shape [batch_size, num_classes, height, width]\n",
    "        inputs = F.softmax(inputs, dim=1)\n",
    "        # Ensure targets are in the expected shape [batch_size, height, width] and are long integers\n",
    "        targets = targets.squeeze(1)\n",
    "        targets = F.one_hot(targets, num_classes=inputs.shape[1]).float()\n",
    "        targets = targets.permute(0, 3, 1, 2)\n",
    "        prediction = inputs\n",
    "        prediction = flatten(prediction) #flatten all dimensions except channel/class\n",
    "        target = flatten(targets)\n",
    "        target = target.float()\n",
    "\n",
    "        if prediction.size(0) == 1:\n",
    "            # for GDL to make sense we need at least 2 channels (see https://arxiv.org/pdf/1707.03237.pdf)\n",
    "            # put foreground and background voxels in separate channels\n",
    "            prediction = torch.cat((prediction, 1 - prediction), dim=0)\n",
    "            target = torch.cat((target, 1 - target), dim=0)\n",
    "        w_l = target.sum(-1)\n",
    "        w_l = 1 / (w_l * w_l).clamp(min=self.epsilon)\n",
    "        w_l.requires_grad = False\n",
    "\n",
    "        intersect = (prediction * target).sum(-1)\n",
    "        intersect = intersect * w_l\n",
    "\n",
    "        denominator = (prediction + target).sum(-1)\n",
    "        denominator = (denominator * w_l).clamp(min=self.epsilon)\n",
    "\n",
    "        return 1 - (2 * (intersect.sum() / denominator.sum()))\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return self.dice_loss(pred, target)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = MyCriterion()\n",
    "\n",
    "# For calculating accuracy\n",
    "def binary_accuracy(preds, targets):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    # output = output.view(1, 1, 512, 512)  # Reshape to original image size\n",
    "    cubes = (preds[:,1,:,:] > 0.5).float()  # Convert probabilities to binary (0 or 1)\n",
    "    correct = (cubes == targets).float()\n",
    "    acc = correct.sum() / correct.numel()\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:09<00:00, 26.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.3767, Accuracy: 0.9490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 30.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15, Loss: 0.0756, Accuracy: 0.9903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 31.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15, Loss: 0.0374, Accuracy: 0.9922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 30.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15, Loss: 0.0322, Accuracy: 0.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 30.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15, Loss: 0.0235, Accuracy: 0.9937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 30.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15, Loss: 0.0207, Accuracy: 0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 30.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15, Loss: 0.0190, Accuracy: 0.9945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 30.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15, Loss: 0.0210, Accuracy: 0.9945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 31.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15, Loss: 0.0174, Accuracy: 0.9948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 31.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15, Loss: 0.0147, Accuracy: 0.9956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 30.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15, Loss: 0.0178, Accuracy: 0.9954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 31.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15, Loss: 0.0129, Accuracy: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 30.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15, Loss: 0.0122, Accuracy: 0.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 30.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Loss: 0.0111, Accuracy: 0.9965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 30.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Loss: 0.0107, Accuracy: 0.9967\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "num_epochs = 15\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for imgs, labels in tqdm(train_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        imgs = imgs.permute(0, 3, 1, 2)  # Change to NCHW format\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = binary_accuracy(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    epoch_acc /= len(train_loader)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), f'./checkpoints/model_epoch{epoch+1}.pth')\n",
    "    if (epoch+1) == num_epochs:\n",
    "        torch.save(model.state_dict(), f'./checkpoints/model_latest.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 60.58it/s]\n"
     ]
    }
   ],
   "source": [
    "class valDataset(Dataset):\n",
    "    def __init__(self, test_dir):\n",
    "        self.test_dir = test_dir\n",
    "        self.name_list = os.listdir(test_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.name_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.test_dir, f'{idx+501}.tif')\n",
    "        img = Image.open(img_path).convert('RGB')  # Convert to grayscale\n",
    "        img = np.array(img).astype('float32')\n",
    "        img = np.array(img)\n",
    "        # Normalize image\n",
    "        img -= img.mean()\n",
    "        img /= img.std()\n",
    "\n",
    "        img_tensor = torch.from_numpy(img).float()\n",
    "        return img_tensor\n",
    "\n",
    "# Load test data\n",
    "val_dataset = valDataset('data/val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load the trained model\n",
    "model = UNet()\n",
    "model.load_state_dict(torch.load('./checkpoints/model_latest.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Perform predictions\n",
    "from torchvision.utils import save_image\n",
    "os.makedirs('data/val_prediction', exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for idx, img in enumerate(tqdm(val_loader)):\n",
    "        img = img.to(device)\n",
    "        img = img.permute(0, 3, 1, 2)\n",
    "        output = model(img)\n",
    "        output = torch.softmax(output, dim=1)\n",
    "        # output = output.view(1, 1, 512, 512)  # Reshape to original image size\n",
    "        output = output.cpu()  # Move to CPU and convert to uint8\n",
    "\n",
    "        # To obtain binary mask\n",
    "        output = (output > 0.5).float()  # Convert probabilities to binary (0 or 1)\n",
    "        output = output * 255.0  # Scale to 0 or 255\n",
    "\n",
    "        # Save the prediction\n",
    "        save_image(output, f'data/val_prediction/{idx}.png')\n",
    "        if (idx+1) % 100 == 0:\n",
    "            print(f'Done: {idx+1}/{len(val_loader)} images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Loaded\n",
      "Crop to N images: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [01:17<00:00,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define input and output directories\n",
    "In_dir = './data/test/'\n",
    "Out_dir = './data/prediction/'\n",
    "os.makedirs(Out_dir, exist_ok=True)\n",
    "model_name = './checkpoints/model_latest.pth'\n",
    "normalization = True # Set this based on your needs\n",
    "\n",
    "# Load the model\n",
    "model = UNet(n_classes=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(torch.load('./checkpoints/model_latest.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Network Loaded\")\n",
    "\n",
    "# Set parameters\n",
    "img_size = 1230 #1146\n",
    "to_size = 512\n",
    "step = 200\n",
    "x_list = [k for k in range(0, img_size - to_size, step)] + [img_size - to_size]\n",
    "y_list = [k for k in range(0, img_size - to_size, step)] + [img_size - to_size]\n",
    "n = len(x_list) * len(y_list)\n",
    "print('Crop to N images:', n)\n",
    "\n",
    "# Get list of images\n",
    "nameListTest = os.listdir(In_dir)\n",
    "output_list = [filename.replace('.tif', '.png') for filename in nameListTest]\n",
    "for i in tqdm(range(len(nameListTest) - 1, -1, -1)):\n",
    "    # Load and preprocess the image\n",
    "    img_path = os.path.join(In_dir, nameListTest[i])\n",
    "    img = Image.open(img_path).convert('RGB')  # Convert to grayscale\n",
    "    img_np = np.array(img).astype('float32')\n",
    "\n",
    "    predictions = torch.zeros((n, to_size, to_size), dtype=torch.float32).to(device)\n",
    "    img_pred = torch.zeros(img_np.shape[:2], dtype=torch.float32).to(device)\n",
    "    img_count = torch.zeros(img_np.shape[:2], dtype=torch.float32).to(device)\n",
    "\n",
    "    # tests_tensor = torch.zeros((n, 1, to_size, to_size), dtype=torch.float32).to(device)\n",
    "\n",
    "    n_temp = 0\n",
    "    for x_mark in x_list:\n",
    "        for y_mark in y_list:\n",
    "            img_temp = img_np[x_mark:x_mark + to_size, y_mark:y_mark + to_size].copy()\n",
    "\n",
    "            if normalization:\n",
    "                img_temp -= img_np.mean()\n",
    "                img_temp /= img_np.std()\n",
    "            else:\n",
    "                img_temp /= 255.0\n",
    "\n",
    "            # Convert to tensor and add channel dimension\n",
    "            img_temp_tensor = torch.from_numpy(img_temp).unsqueeze(0)  # Shape: (1, to_size, to_size, 3)\n",
    "            img_temp_tensor = img_temp_tensor.permute(0, 3, 1, 2)  # Change to NCHW format\n",
    "            img_temp_tensor = img_temp_tensor.to(device)\n",
    "            with torch.no_grad():\n",
    "                predictions[n_temp] = torch.softmax(model(img_temp_tensor), dim=1)[:, 1, :, :]\n",
    "            n_temp += 1\n",
    "        predictions = (predictions > 0.5).float()\n",
    "    # Assemble the predictions back into the image\n",
    "    n_temp = 0\n",
    "    for x_mark in x_list:\n",
    "        for y_mark in y_list:\n",
    "            img_pred[x_mark:x_mark + to_size, y_mark:y_mark + to_size] += predictions[n_temp]\n",
    "            img_count[x_mark:x_mark + to_size, y_mark:y_mark + to_size] += 1\n",
    "            n_temp += 1\n",
    "\n",
    "    # Normalize the predictions\n",
    "    img_pred /= img_count\n",
    "    # img_pred = (img_pred > 0.5).float()\n",
    "    # Convert to numpy array and save image\n",
    "    img_pred_np = img_pred.cpu().numpy()\n",
    "\n",
    "    # Normalize to 0-255 and convert to uint8\n",
    "    img_pred_np -= img_pred_np.min()\n",
    "    img_pred_np /= img_pred_np.max()\n",
    "    img_pred_np = (img_pred_np*255).astype(np.uint8)\n",
    "\n",
    "    # Save the image\n",
    "    img_out = Image.fromarray(img_pred_np)\n",
    "    out_path = os.path.join(Out_dir, output_list[i])\n",
    "    img_out.save(out_path)\n",
    "    # print('Image saved:', nameListTest[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
